import streamlit as st
from model.MyLlama import Transformer, LLMConfig
from transformers import AutoTokenizer
import torch
from llm_tokenizer.utils import ChatMLSFT
from llm_tokenizer.utils import SimpleStreamDecoder

def get_model_tokenizer(device):
    config = LLMConfig()
    model = Transformer(config).to(device)
    tokenizer = AutoTokenizer.from_pretrained("./llm_tokenizer")
    return model, tokenizer
device = "cuda:0"
model, tokenizer = get_model_tokenizer(device)
model.load_state_dict(torch.load("checkpoints/model_512_2.pth"))

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(page_title="Hello LLM!",page_icon=":man-surfing:")

# åˆå§‹åŒ–èŠå¤©å†å²
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "ä½ å¥½ï¼æˆ‘æ˜¯ä¸€ä¸ªä¸“æ³¨å†™å¤è¯—çš„æœºå™¨äººï¼Œè¯·è¾“å…¥è¯—æ­Œé£æ ¼å’Œæ¨¡ä»¿çš„è¯—äººè¿›è¡Œåˆ›ä½œï¼ä¾‹å¦‚ï¼šè¯·ä½ æ¨¡ä»¿å½­ç»©çš„é£æ ¼ï¼Œåˆ›ä½œä¸€é¦–å¤è¯—è¯ï¼Œè¡¨è¾¾å¯¹äº²å‹çš„æ¬£èµå’ŒæœŸå¾…ã€‚è¯·æ³¨é‡è¯­è¨€çš„è´¨æœ´å’Œæƒ…æ„Ÿçš„çœŸæŒšã€‚"}]

# å®šä¹‰å“åº”ç”Ÿæˆå™¨ï¼ˆæ¨¡æ‹Ÿæ‰“å­—æœºæ•ˆæœï¼‰
def generate_response(messages):
    # è¿™é‡Œæ˜¯æ¨¡æ‹Ÿå“åº”ï¼Œå®é™…åº”ç”¨ä¸­å¯æ›¿æ¢ä¸ºçœŸå®çš„AIæ¨¡å‹APIè°ƒç”¨
    # å®é™…ä¸Šè¦è°ƒç”¨æœ€è¿‘çš„å‡ ä¸ªèŠå¤©è®°å½•
    prompt = ChatMLSFT(messages,True)
    decoder = SimpleStreamDecoder(tokenizer)
    enc = tokenizer(prompt,
                    truncation=True,
                    return_tensors='pt')
    full_response = ""
    for next_token in model.generate_stream(input_ids=enc["input_ids"].to(device),
                   attention_mask=enc["attention_mask"].to(device),
                   max_length=1024,
                   eos_token_id=tokenizer.encode("<|im_end|>")[0]):
        new_text = decoder.decode(next_token.squeeze(0).item())
        if new_text == "<|im_end|>":
            break
        if new_text:
            full_response += new_text
            yield new_text
            
    return full_response
    

# # é¡µé¢æ ‡é¢˜
# st.title("ğŸ¤– å¤šè½®èŠå¤©åº”ç”¨æ¼”ç¤º")

# æ˜¾ç¤ºå†å²æ¶ˆæ¯
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ç”¨æˆ·è¾“å…¥
if prompt := st.chat_input("Say something to LLM"):
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # ç”ŸæˆåŠ©æ‰‹å“åº”
    with st.chat_message("assistant"):
        # response = st.write_stream(response_generator(prompt))
        message_placeholder = st.empty()
        full_response = ""
        
        # è°ƒç”¨ç”Ÿæˆå™¨è·å–æµå¼å“åº”
        for chunk in generate_response(st.session_state.messages):
            full_response += chunk
            message_placeholder.markdown(full_response + "â–Œ")
        
        message_placeholder.markdown(full_response)
    
    # æ·»åŠ åŠ©æ‰‹å“åº”åˆ°å†å²
    st.session_state.messages.append({"role": "assistant", "content": full_response})

# åˆ›å»ºæ›´ä¸°å¯Œçš„ç•Œé¢å¸ƒå±€
st.sidebar.title("èŠå¤©è®¾ç½®")

# æ·»åŠ èŠå¤©æ§åˆ¶é€‰é¡¹
clear_chat = st.sidebar.button("æ¸…ç©ºèŠå¤©è®°å½•")
if clear_chat:
    st.session_state.messages = []
    st.rerun()

# # æ·»åŠ æ ·å¼ç¾åŒ–
# st.markdown("""
# <style>
#     .stChatMessage {
#             padding: 1rem;
#             border-radius: 0.5rem;
#             margin-bottom: 1rem;
#         }
#     .stChatInput {
#         position: fixed;
#         bottom: 20px;
#     }
# </style>
# """, unsafe_allow_html=True)
